paper_id,speaker,title,,abstract
1,M. I. JORDAN,"Machine learning: Trends, perspectives, and prospects",,"Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing."
2,JULIA HIRSCHBERG,Advances in natural language processing,,"Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today’s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area."
3,Steven K. Thompson,Adaptive Cluster Sampling,,"In many real-world sampling situations, researchers would like to be able to adaptively increase sampling effort in the vicinity of observed values that are high or otherwise interesting. This article describes sampling designs in which, whenever an observed value of a selected unit satisfies a condition of interest, additional units are added to the sample from the neighborhood of that unit. If any of these additional units satisfies the condition, still more units may be added. Sampling designs such as these, in which the selection procedure is allowed to depend on observed values of the variable of interest, are in contrast to conventional designs, in which the entire selection of units to be included in the sample may be determined prior to making any observations. Because the adaptive selection procedure introduces biases into conventional estimators, several estimators are given that are design unbiased for the population mean with the adaptive cluster designs of this article; that is, the unbiasedness does not depend on any assumptions about the population. The Rao-Blackwell method is used to obtain improved unbiased estimators; because of the incompleteness of the minimal sufficient statistic, more than one of these improved estimators are obtained. Simple criteria are given determining when adaptive cluster sampling strategies are more efficient than simple random sampling of equivalent sample size. Motivation for the designs in this article is provided by a wide variety of sampling situations in fields such as ecology, geology, and epidemiology. For example, in a survey of a rare bird species, once individuals of the species are detected, additional observations at nearby sites often reveal more individuals. In a study of a contagious disease, the addition to the sample of close contacts of infected individuals reveals a higher than average incidence rate. The results and examples in this article show that adaptive cluster sampling strategies give lower variance than conventional strategies for certain types of populations and, in particular, provide an extremely effective way of sampling rare, clustered populations."
4,Steven K. Thompson,Sample Size for Estimating Multinomial Proportions,,"This article presents a procedure and a table for selecting sample size for simultaneously estimating the parameters of a multinomial distribution. The results are obtained by examining the “worst” possible value of a multinomial parameter vector, analogous to the case in which a binomial parameter equals one-half."
5,Rich Caruana,An empirical comparison of supervised learning algorithms,,"A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods."
6,Rich Caruana,Predicting good probabilities with supervised learning,,"We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities."
7,Eric Baum,Supervised Learning of Probability Distributions by Neural Networks,,"We propose that the back propagation algorithm for supervised learning can be generalized, put on a satisfactory conceptual footing, and very likely made more efficient by defining the val(cid:173) ues of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood, rather than the 'error'. In the past thirty years many researchers have studied the question of supervised learning in 'neural'-like networks. Recently a learning algorithm called 'back propagation H - 4 or the 'general(cid:173) ized delta-rule' has been applied to numerous problems including the mapping of text to phonemes 5 , the diagnosis of illnesses6 and the classification of sonar targets 7 • In these applications, it would often be natural to consider imperfect, or probabilistic informa(cid:173) tion. We believe that by considering supervised learning from this slightly larger perspective, one can not only place back propaga"
8,Farzan Farnia,A Minimax Approach to Supervised Learning,,"Given a task of predicting Y from X, a loss function L, and a set of probability distributions Gamma on (X,Y), what is the optimal decision rule minimizing the worst-case expected loss over Gamma? In this paper, we address this question by introducing a generalization of the maximum entropy principle. Applying this principle to sets of distributions with marginal on X constrained to be the empirical marginal, we provide a minimax interpretation of the maximum likelihood problem over generalized linear models as well as some popular regularization schemes. For quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models. Moreover, for the 0-1 loss we derive a classifier which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss over the proposed Gamma by solving a tractable optimization problem. We perform several numerical experiments to show the power of the minimax SVM in outperforming the SVM."
9,Yupan Huang ,LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking,,"Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3."
10,Young-Guk Ha,Image Preprocessing for Efficient Training of YOLO Deep Learning Networks,,"Every artificial intelligence system needs big data for training. In particular, artificial intelligence for object detection requires a lot of images for training. It is possible to obtain training images from internet by web crawler. In most cases, however, images collected by the crawler are often not refined. In other words, it can't be used as training data in any artificial intelligence platform without proper pre-processing. This paper describes a pre-processing system for images collected by a crawler for YOLO training. And it will proved that this system is capable of efficient training."
11,Kevin A. Kwiat,Game Theory for Cyber Security and Privacy,,"In this survey, we review the existing game-theoretic approaches for cyber security and privacy issues, categorizing their application into two classes, security and privacy. To show how game theory is utilized in cyberspace security and privacy, we select research regarding three main applications: cyber-physical security, communication security, and privacy. We present game models, features, and solutions of the selected works and describe their advantages and limitations from design to implementation of the defense mechanisms. We also identify some emerging trends and topics for future research. This survey not only demonstrates how to employ game-theoretic approaches to security and privacy but also encourages researchers to employ game theory to establish a comprehensive understanding of emerging security and privacy problems in cyberspace and potential solutions."
12,Galya Pavlova,Cyber security: Threats and Challenges,,"Nowadays, computer and network system maintenance is just as important as their protection. Grow into the network systems that are posted on many different websites with payment function and this is seen as websites needed sensitive information and are important to any organization. Viruses represent one of the most successful ways to attack these systems and their use is inevitably part of the computer business worldwide. The use of identification and/or verification must be protected by the recommendable antivirus software, firewalls etc as well as by multiple other technologies created to protect the trinity of information security: confidentiality, integrity and availability. There is no specific standardized procedure for designing a secure network. Network security must be tailored to the current organization needs and achieve the appropriate protection required. In this article we examine and summarize the most popular and successful means of protections used by organizations as well as by individuals."
13,Wahab Almuhtadi,Malware Detection and Security Analysis Capabilities in a Continuous Integration ,,"Risk management is an essential part of software security. Assemblyline is a software security tool developed by the Canadian Centre for Cyber Security (CCCS) for malware detection and analysis. In this paper, we examined the performance of Assemblyline for assessing the risk of executable files. We developed and examined use-cases where Assemblyline is included as part of a security safety net assessing vulnerabilities that would lead to risk. Finally, we considered Assemblyline’s utility in a continuous integration / delivery context using our test results."
14,Yuri Baturin,"Computer Crimes, Computer Security and Computer Law",,"The article describes the practice of applying laws on computer crimes in the Russian Federation, as well as the situation with computer security and the training of specialists in the field of computer law."
15,Michael Losavio,STEM for Public Safety in Cyber: Training for Local Law Enforcement and Cyber Security,,"Public safety and cybersecurity are integrated endeavors as pervasive computing puts interconnected devices everywhere. STEM training in computing and cyber security might occur within the traditional public safety framework, both of criminological study and practical implementation. We tested the responsiveness to cybersecurity training for law enforcement from this traditional perspective. Our findings suggest great potential within this group for expanding effective cyber security protections for communities through STEM computing and cyber security education. They further suggest a commensurate need for training and support to realize this potential."
16,Shaikha Hasan,Evaluating the cyber security readiness of organizations and its influence on performance,,"The acceleration of cyber-attacks in recent years has negatively impacted the overall performance of organizations around the world. Organizations face the challenge of enhancing their cyber security to prevent and combat cyber-attacks, but studies of factors impacting the cyber security awareness/readiness of organizations from a holistic perspective are lacking. This study adopts the Technology-Organization-Environment (TOE) framework to examine a comprehensive set of factors influencing the cyber security readiness of organizations and the effects of these factors on organizational performance (financial and non-financial) mediated by improved organizational security performance. Data are collected via a survey of IT professionals in Bahrain, with 270 valid responses. The results confirm the importance of seven of nine factors affecting cyber security readiness identified in previous studies. Cyber security readiness is found to positively impact organizational security performance, which in turn positively affects financial and non-financial performance. The newly proposed comprehensive model of factors affecting the cyber security readiness of organizations and the evidence of their importance can be used to guide future research and enhance the current understanding of how organizations can better equip themselves to minimize the occurrence and impact of cyber-attacks."
17,Peng Xie,Using Bayesian networks for cyber security analysis,,"Capturing the uncertain aspects in cyber security is important for security analysis in enterprise networks. However, there has been insufficient effort in studying what modeling approaches correctly capture such uncertainty, and how to construct the models to make them useful in practice. In this paper, we present our work on justifying uncertainty modeling for cyber security, and initial evidence indicating that it is a useful approach. Our work is centered around near real-time security analysis such as intrusion response. We need to know what is really happening, the scope and severity level, possible consequences, and potential countermeasures. We report our current efforts on identifying the important types of uncertainty and on using Bayesian networks to capture them for enhanced security analysis. We build an example Bayesian network based on a current security graph model, justify our modeling approach through attack semantics and experimental study, and show that the resulting Bayesian network is not sensitive to parameter perturbation."
18,Joseph Da Silva,Cyber security and the Leviathan,,"Dedicated cyber-security functions are common in commercial businesses, who are confronted by evolving and pervasive threats of data breaches and other perilous security events. Such businesses are enmeshed with the wider societies in which they operate. Using data gathered from in-depth, semi-structured interviews with 15 Chief Information Security Officers, as well as six senior organisational leaders, we show that the work of political philosopher Thomas Hobbes, particularly Leviathan, offers a useful lens through which to understand the context of these functions and of cyber security in Western society. Our findings indicate that cyber security within these businesses demonstrates a number of Hobbesian features that are further implicated in, and provide significant benefits to, the wider Leviathan-esque state. These include the normalisation of intrusive controls, such as surveillance, and the stimulation of consumption. We conclude by suggesting implications for cyber-security practitioners, in particular, the reflexivity that these perspectives offer, as well as for businesses and other researchers."
19,Antonio Paya,Egida: Automated security configuration deployment systems with early error detection,,"Automated deployment of validated security controls is very important to implement defense-in-depth strategies to secure machine infrastructures. This paper describes a technique to perform these deployments in a more controlled way, using a DSL. According to the target machine configuration or characteristics, the DSL provides ways of deploying automated security controls that are less prone to negatively impact legitimate running services, and allows detecting several deployment errors earlier. This helps to better capture system administrator expert knowledge, and share automated security scripts that obtain better hardening results. The initial results of this technique are promising enough to apply them on educational environments, and can be further developed to be applied on production infrastructures."
20,Jens Van den Broeck,Flexible software protection,,"To counter software reverse engineering or tampering, software obfuscation tools can be used. However, such tools to a large degree hard-code how the obfuscations are deployed. They hence lack resilience and stealth in the face of many attacks. To counter this problem, we propose the novel concept of flexible obfuscators, which implement protections in terms of data structures and APIs already present in the application to be protected. The protections are hence tailored to the application in which they are deployed, making them less learnable and less distinguishable. In our research, we concretized the flexible protection concept for opaque predicates. We designed an interface to enable the reuse of existing data structures and APIs in injected opaque predicates, we analyzed their resilience and stealth, we implemented a proof-of-concept flexible obfuscator, and we evaluated it on a number of real-world use cases. This paper presents an in-depth motivation for our work, the design of the interface, an in-depth security analysis, and a feasibility report based on our experimental evaluation. The findings are that flexible opaque predicates indeed provide strong resilience and improved stealth, but also that their deployment is costly, and that they should hence be used sparsely to protect only the most security-sensitive code fragments that do not dominate performance. Flexible obfuscation therefor delivers an expensive but also more durable new weapon in the ever ongoing software protection arms race."
21,Carlotta Pucci,Innovative approaches for cancer treatment: current perspectives and new challenges,,"Every year, cancer is responsible for millions of deaths worldwide and, even though much progress has been achieved in medicine, there are still many issues that must be addressed in order to improve cancer therapy. For this reason, oncological research is putting a lot of effort towards finding new and efficient therapies which can alleviate critical side effects caused by conventional treatments. Different technologies are currently under evaluation in clinical trials or have been already introduced into clinical practice. While nanomedicine is contributing to the development of biocompatible materials both for diagnostic and therapeutic purposes, bioengineering of extracellular vesicles and cells derived from patients has allowed designing ad hoc systems and univocal targeting strategies. In this review, we will provide an in-depth analysis of the most innovative advances in basic and applied cancer research."
22,Francisco Rodrígues,Nano-Based Approved Pharmaceuticals for Cancer Treatment: Present and Future Challenges,,"Cancer is one of the main causes of death worldwide. To date, and despite the advances in conventional treatment options, therapy in cancer is still far from optimal due to the non-specific systemic biodistribution of antitumor agents. The inadequate drug concentrations at the tumor site led to an increased incidence of multiple drug resistance and the appearance of many severe undesirable side effects. Nanotechnology, through the development of nanoscale-based pharmaceuticals, has emerged to provide new and innovative drugs to overcome these limitations. In this review, we provide an overview of the approved nanomedicine for cancer treatment and the rationale behind their designs and applications. We also highlight the new approaches that are currently under investigation and the perspectives and challenges for nanopharmaceuticals, focusing on the tumor microenvironment and tumor disseminate cells as the most attractive and effective strategies for cancer treatments."
23,DAVID CROSBY,Early detection of cancer,,"Many types of cancer are detected at an advanced stage, when treatment options are limited and prognosis is poor. Being able to detect cancers early can substantially improve survival, but this approach comes with challenges, including the possibility of overdiagnosis and overtreatment, which can harm people who would not have developed overt malignancy. In a Review, Crosby et al. discuss the importance of cancer early detection and the main challenges that need to be overcome to better understand the early events in tumorigenesis that are detectable in screening tests. The results of these tests can then be reliably interpreted to determine whether an individual requires treatment. —GKA"
24,Pranav Rajpurkar,AI in health and medicine,,"Artificial intelligence (AI) is poised to broadly reshape medicine, potentially improving the experiences of both clinicians and patients. We discuss key findings from a 2-year weekly effort to track and share key developments in medical AI. We cover prospective studies and advances in medical image analysis, which have reduced the gap between research and deployment. We also address several promising avenues for novel medical AI research, including non-image data sources, unconventional problem formulations and human–AI collaboration. Finally, we consider serious technical and ethical challenges in issues spanning from data scarcity to racial bias. As these challenges are addressed, AI’s potential may be realized, making healthcare more accurate, efficient and accessible for patients worldwide."
25,Edward H. Shortliffe,The adolescence of AI in Medicine: Will the field come of age in the '90s?,,"Artificial intelligence in medicine (AIM) has reached a period of adolescence in which interactions with the outside world are not only natural but mandatory. Although the basic research topics in AIM may be those of artificial intelligence, the applied issues touch more generally on the broad field of medical informatics. To the extent that AIM research is driven by performance goals for biomedicine, AIM is simply one component within a wide range of research and development activities. Furthermore, an adequate appraisal of AIM research requires an understanding of the research motivations, the complexity of the problems, and a suitable definition of the criteria for judging the field's success. Effective fielding of AIM systems will be dependent on the development of integrated environments for communication and computing that allow merging of knowledge-based tools with other patient data-management and information-retrieval applications. The creation of this kind of infrastructure will require vision and resources from leaders who realize that the practice of medicine is inherently an information-management task and that biomedicine must make the same kind of coordinated commitment to computing technologies as have other segments of our society in which the importance of information management is well understood."
26,Ronald Pyram,Chronic kidney disease and diabetes,,"Chronic kidney disease has a significant worldwide prevalence affecting 7.2% of the global adult population with the number dramatically increasing in the elderly. Although the causes are various, diabetes is the most common cause of CKD in the United States and an increasing cause of the same worldwide. Therefore, we chose to focus on diabetic chronic kidney disease in this review."
27,Christiane Reitz,Epidemiology of Alzheimer disease,,"The global prevalence of dementia is estimated to be as high as 24 million, and is predicted to double every 20 years through to 2040, leading to a costly burden of disease. Alzheimer disease (AD) is the leading cause of dementia and is characterized by a progressive decline in cognitive function, which typically begins with deterioration in memory. Before death, individuals with this disorder have usually become dependent on caregivers. The neuropathological hallmarks of the AD brain are diffuse and neuritic extracellular amyloid plaques—which are frequently surrounded by dystrophic neurites—and intracellular neurofibrillary tangles. These hallmark pathologies are often accompanied by the presence of reactive microgliosis and the loss of neurons, white matter and synapses. The etiological mechanisms underlying the neuropathological changes in AD remain unclear, but are probably affected by both environmental and genetic factors. Here, we provide an overview of the criteria used in the diagnosis of AD, highlighting how this disease is related to, but distinct from, normal aging. We also summarize current information relating to AD prevalence, incidence and risk factors, and review the biomarkers that may be used for risk assessment and in diagnosis."
28,Dennis L. Polla,Microdevices in Medicine,,"The application of microelectromechanical systems (MEMS) to medicine is described. Three types of biomedical devices are considered, including diagnostic microsystems, surgical microsystems, and therapeutic microsystems. The opportunities of MEMS miniaturization in these emerging disciplines are considered, with emphasis placed on the importance of the technology in providing a better outcome for the patient and a lower overall health care cost. Several case examples in each of these areas are described. Key aspects of MEMS technology as it is applied to these three areas are described, along with some of the fabrication challenges."
29,Yoav Mintz,Introduction to artificial intelligence in medicine,,"AI is integrated into our daily lives in many forms, such as personal assistants (Siri, Alexa, Google assistant etc.), automated mass transportation, aviation and computer gaming. More recently, AI has also begun to be incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy, opening the path to providing better healthcare overall. Radiological images, pathology slides, and patients’ electronic medical records (EMR) are being evaluated by machine learning, aiding in the process of diagnosis and treatment of patients and augmenting physicians’ capabilities. Herein we describe the current status of AI in medicine, the way it is used in the different disciplines and future trends."
30,Nicolas Padoy,Machine and deep learning for workflow recognition during surgery,,"Recent years have seen tremendous progress in artificial intelligence (AI), such as with the automatic and real-time recognition of objects and activities in videos in the field of computer vision. Due to its increasing digitalization, the operating room (OR) promises to directly benefit from this progress in the form of new assistance tools that can enhance the abilities and performance of surgical teams. Key for such tools is the recognition of the surgical workflow, because efficient assistance by an AI system requires this system to be aware of the surgical context, namely of all activities taking place inside the operating room. We present here how several recent techniques relying on machine and deep learning can be used to analyze the activities taking place during surgery, using videos captured from either endoscopic or ceiling-mounted cameras. We also present two potential clinical applications that we are developing at the University of Strasbourg with our clinical partners."
31,Caleb T. Carr,"Social Media: Defining, Developing, and Divining",,"What is a social medium, and how may one moderate, isolate, and influence communicative processes within? Although scholars assume an inherent understanding of social media based on extant technology, there is no commonly accepted definition of what social media are, both functionally and theoretically, within communication studies. Given this lack of understanding, cogent theorizing regarding the uses and effects of social media has been limited. This work first draws on extant definitions of social media and subcategories (e.g., social network sites) from public relations, information technology, and management scholarship, as well as the popular press, to develop a definition of social media precise enough to embody these technologies yet robust enough to remain applicable in 2035. It then broadly explores emerging developments in the features, uses, and users of social media for which future theories will need to account. Finally, it divines and prioritizes challenges that may not yet be apparent to theorizing communication processes with and in mercurial social media. We address how social media may uniquely isolate and test communicative principles to advance our understanding of human–human and human–computer interaction. In all, this article provides a common framework to ground and facilitate future communication scholarship and beyond."
32,Marijke De Veirman,instagram: the impact of number of followers and product divergence on brand attitude,,"indings of two experimental studies show that Instagram influencers with high numbers of followers are found more likeable, partly because they are considered more popular. Important, only in limited cases, perceptions of popularity induced by the influencer's number of followers increase the influencer's perceived opinion leadership. However, if the influencer follows very few accounts him-/herself, this can negatively impact popular influencers’ likeability. Also, cooperating with influencers with high numbers of followers might not be the best marketing choice for promoting divergent products, as this decreases the brand's perceived uniqueness and consequently brand attitudes."
33,Sitaram Asur,Predicting the Future with Social Media,,"In recent years, social media has become ubiquitous and important for social networking and content sharing. And yet, the content that is generated from these websites remains largely untapped. In this paper, we demonstrate how social media content can be used to predict real-world outcomes. In particular, we use the chatter from Twitter.com to forecast box-office revenues for movies. We show that a simple model built from the rate at which tweets are created about particular topics can outperform market-based predictors. We further demonstrate how sentiments extracted from Twitter can be utilized to improve the forecasting power of social media."
34,Ana Lucia Schmidt,The COVID-19 social media infodemic,,"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction number ??0 for each social media platform. Moreover, we identify information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors’ amplification."
35,Georgios Tsimonis,Brand strategies in social media,,"The purpose of this paper is to: first, examine why companies create brand pages in social media, how they use them, what policies and strategies they follow, and what outcomes do they expect; and second – from firms’ point of view – how users are benefited from such pages."
36,Bertrand Marianne,New Perspectives on Gender,,"Psychological and socio-psychological factors are now more commonly discussed as possible explanations for gender differences in labor market outcomes. We first describe the (mainly) laboratory-based evidence regarding gender differences in risk preferences, in attitudes towards competition, in the strength of other-regarding preferences, and in attitudes towards negotiation. We then review the research that has tried to quantify the relevance of these factors in explaining gender differences in labor market outcomes outside of the laboratory setting. We also describe recent research on the relationship between social and gender identity norms and women’s labor market choices and outcomes, as well as on the role of child-rearing practices in explaining gender identity norms. Finally, we report on some recent work documenting puzzling trends in women’s well-being and discuss possible explanations for these trends, including identity considerations. We conclude with suggestions for future research."
37,Elisa Keller,Labor supply and gender differences in occupational choice,,"This paper uses data on the task content of occupations to study the role of labor supply in occupational choice. In 1970, married women were less likely to choose occupations characterized by analytically intensive tasks than were men. By 2010, gender differences in occupational choice had narrowed significantly. I use the Dictionary of Occupational Titles to measure the value of skill in an occupation and find an increase in this value with the analytical intensity of occupational tasks. I argue that, as a significant part of skill is accumulated on the job, sources that encourage women to commit to market work contributed to the gender convergence in occupational choice. A quantitative exercise measures that labor-saving technical change in the household sector, occupation-biased technical change in final good production, declining gender gaps in wages and schooling account for 58% of the gender convergence in occupational choice, via the labor supply channel."
38,James Albrecht,The career dynamics of high-skilled women and men: Evidence from Sweden,,"In this paper, we use matched worker-firm register data from Sweden to examine the career dynamics of high-skill women and men. Specifically, we track wages for up to 20 years among women and men born in the years 1960–70 who completed a university degree in business or economics. These women and men have similar wages and earnings at the start of their careers, but their career paths diverge substantially as they age. These men and women also have substantial differences in wage paths associated with becoming a parent. We look at whether firm effects account for the differences we observe between women’s and men’s wage profiles. We document differences between the firms where men work and those where women work. However, a wage decomposition suggests that these differences in firm characteristics play only a small role in explaining the gender log wage gap among these workers. We then examine whether gender differences in firm-to-firm mobility help explain the patterns in wages that we see. Men and women both exhibit greater mobility early in their careers, but there is little gender difference in this firm-to-firm mobility. We find that the main driver of the gender difference in log wage profiles is that men experience higher wage gains than women do both as “switchers” and as “stayers.”"
39,Jorge Luis García,Gender differences in the benefits of an influential early childhood program,,"This paper studies the life-cycle impacts of a widely emulated high-quality, intensive early childhood program with long-term follow up. The program starts early in life (at 8 weeks of age) and is evaluated by an RCT. There are multiple treatment effects which we summarize through interpretable aggregates. Girls have a greater number of statistically significant treatment effects than boys and effect sizes for them are generally bigger. The source of this difference is worse home environments for girls with greater scope for improvement by the program. Fathers of sons support their families more than fathers of daughters."
40,Marco Francesconi,Early gender gaps among university graduates,,"We use data from six cohorts of university graduates in Germany to assess the extent of gender gaps in college and labor market performance twelve to eighteen months after graduation. Men and women enter college in roughly equal numbers, but more women than men complete their degrees. Women enter college with slightly better high school grades, but women leave university with slightly lower marks. Immediately following university completion, male and female full-timers work a very similar number of hours per week, but men earn more than women across the pay distribution, with an unadjusted gender gap in full-time monthly earnings of about 20 log points on average. Including a large set of controls reduces the gap to 5–10 log points. The single most important proximate factor that explains the gap is field of study at university."
